% main.tex
\documentclass[11pt]{article}
\usepackage{pepadun}

% \usepackage[backend=biber, style=ieee]{biblatex}
% \addbibresource{pepadun.bib}  % Gunakan file bib yang disediakan

% Metadata dokumen
\title{Template Jurnal Pepadun}
\author{%
    \begin{tabular}{c}
        \textsuperscript{1}First Author, \\
        \textsuperscript{2}Next Author and \\
        \textsuperscript{3}Last Author \\[6pt]
        \textsuperscript{1,2}Department name, institution name, \\
        \textsuperscript{3}Department name, institution name, \\ 
        Institution address, city, country \\[6pt]
        e-mail: \textsuperscript{1} xxx@yyy.zzz, \\
        \textsuperscript{2} \href{mailto:xxx@yyy.zzz}{\uline{xxx@yyy.zzz}}
    \end{tabular}%
}
\date{} % Hapus tanggal otomatis

\begin{document}

\maketitle

\begin{abstract}
TODO
\end{abstract}

\keywords{RL, DL, ABM}

\section{INTRODUCTION}

Reinforcement learning (RL) has recently been explored as a tool for macroeconomic modeling, but current approaches fall into two broad categories, each with notable limitations. (1) Single-agent RL with a representative agent: Here a single RL "agent" represents an entire sector (e.g. the representative household or firm) interacting with an abstract macroeconomic environment. This approach simplifies the economy but often requires very restrictive assumptions. It struggles to handle complex lifetime utility maximization with expectations, so researchers are forced to use overly simplified representative agents.

Moreover, because most of the economy is baked into the fixed environment dynamics, these models lack rich agent dynamics – making it difficult to capture how policy changes propagate over time through agent behavior. (2) Multi-agent RL with many agents: In this approach, multiple agents (households, firms, government, etc.) are each modeled as RL agents. While this allows heterogeneity and potentially richer interactions, multi-agent RL systems are notoriously hard to train. They often suffer from instability and have no guarantee of converging to a stable equilibrium as the number of agents grows
jmlr.org
. This instability severely limits scalability. Yet, to study crucial macro questions like income inequality or optimal taxation, we need to simulate a large population of heterogeneous agents – something that neither single-agent nor naive multi-agent RL handles well under current methods.

To address these challenges, we propose a new framework called the Semi-RL Autonomous Macro System (SAMS). This hybrid approach combines the strengths of traditional economic modeling with modern RL and deep learning techniques. The key idea is to let the majority of agents in the model behave according to established economic principles (following their first-order conditions, or F.O.C., for optimality) while reserving RL for the elements of the system that we want to learn or optimize (such as a policy-making agent). In other words, instead of making every agent an RL learner (which causes learning instability) or reducing the whole economy to one trivial RL agent (which oversimplifies dynamics), we split the work: non-RL agents follow adaptive decision rules derived from economic theory, and a limited number of RL agents learn optimal policies within this environment. The non-RL agents are not static; they behave autonomously by optimizing their objectives (e.g. utility or profit) via their F.O.C., which means they respond realistically to changes in the economic environment. This creates a dynamic and responsive macroeconomic simulator, but without the explosion of learning complexity that plagues full MARL systems. 

Technically, our framework leverages a Monte Carlo simulation-based deep learning method to solve these agents’ optimization problems. Recent advances in deep learning have shown that we can efficiently solve high-dimensional dynamic models (such as heterogeneous-agent macro models) by casting them as neural-network approximations to Bellman or Euler equations
web.stanford.edu
. In particular, we build on the method of Maliar et al. (2021), who demonstrated a deep learning approach capable of handling large-scale heterogeneous-agent models (e.g. the Krusell–Smith model with thousands of agents) without resorting to the usual simplifying assumptions
web.stanford.edu
. By incorporating this deep learning Monte Carlo solver, SAMS frees us from having to assume a single representative agent. We can include a rich variety of heterogeneous agents whose decision rules are computed through the deep learning framework, ensuring that each agent (or type of agent) optimally responds to its economic incentives and constraints. These agents effectively simulate the rational behavior one would expect in a macroeconomic model, but with computational tractability even in large numbers.

Meanwhile, the elements of the system that do use RL (for example, a policy agent like a government adjusting tax rates) can learn and adapt their strategies over time. Because the bulk of the economy (households, firms, etc.) is handled by the deep-learning-based F.O.C. solvers, the overall simulation remains much more stable. The environment that the RL agent interacts with is grounded in equilibrium behavior (through the non-RL agents), which is less non-stationary than an all-RL multi-agent system. As a result, SAMS can scale to a large number of heterogeneous agents without suffering the divergence or chaos that typically occurs in multi-agent RL. This hybrid design significantly improves stability and realism: policy effects can be observed through the adaptive responses of many simulated agents, yet the training process remains feasible. <be>

In summary, the Semi-RL Autonomous Macro System offers a balanced approach that bridges the gap between existing RL-in-macro methods. It retains the rich dynamics and heterogeneity of agent-based models while constraining the learning problem to a manageable scope. By focusing RL where it is most needed (e.g. learning optimal policy) and using deep learning to handle traditional agent optimization, this framework aims to enable robust macroeconomic simulations for scenarios like inequality, taxation, and beyond. We anticipate that SAMS can unlock more realistic and scalable macroeconomic policy analysis, combining the strengths of reinforcement learning with the rigor of dynamic economic modeling.


\section{RESEARCH METHODOLOGY}

We develop a hybrid framework that combines deep-learning solutions to dynamic economic optimization with a targeted reinforcement learning (RL) policy agent. RL is reserved solely for the government, while households (and firms, if included) are modeled as optimizing agents whose decision rules are computed via a neural network (NN) solver that minimizes Euler-equation residuals. This preserves equilibrium discipline and heterogeneity without incurring the instability typical of full multi-agent RL.

\paragraph{Stage 1: Economic environment construction.}
We specify a backbone macroeconomic model with heterogeneous households, preference and technology parameters, market structure, and aggregate shocks. Each household type’s policy function is represented by a neural network $f_{\phi}$ that maps states (idiosyncratic and aggregate) into decisions (e.g., consumption, savings, labor). Rather than learning from demonstrations, $f_{\phi}$ is trained by minimizing Euler residuals implied by agents’ first-order conditions, following \cite{maliar2021deep}. This is akin to an imitation-learning analogue in which the ``expert'' is the model’s optimality conditions rather than data \cite{ho2016generative}. Monte Carlo simulation over the joint state distribution enables scalable training across high-dimensional state spaces and rich heterogeneity.

\paragraph{Stage 2: Equilibrium-consistent environment dynamics.}
Given $f_{\phi}$, we simulate the cross-section of households and aggregate to market-level objects (e.g., factor supplies, demands). Prices or wedges are updated by an aggregator (e.g., market-clearing or reduced-form pricing map), delivering a dynamic, equilibrium-consistent environment that responds to policy changes. We iterate between (i) updating $f_{\phi}$ by minimizing Euler residuals under current prices/policy and (ii) recomputing aggregates until residuals and aggregates stabilize within tolerance.

\paragraph{Stage 3: RL-based policy optimization.}
With a responsive, equilibrium-grounded environment in place, we introduce the government RL agent. The agent chooses a policy vector (e.g., tax instruments) $\pi_{\theta}(s_t)$ conditional on the aggregate state $s_t$ (and allowed observables), receives a welfare-based reward (e.g., social welfare with inequality weights or Ramsey objective), and observes next-period outcomes generated by the environment. We train $\pi_{\theta}$ (e.g., via PPO/SAC) on trajectories produced by interacting with the environment that is driven by households’ F.O.C.-consistent behavior. Because household dynamics are stabilized by the Euler-residual solver, the policy learning problem is substantially less non-stationary and more interpretable than in full MARL.

\medskip
\noindent
Overall, the system alternates between (i) solving households’ optimization via Euler-residual minimization and (ii) improving the government’s policy via RL, yielding a Semi-RL Autonomous Macro System that is both scalable and theoretically disciplined.

% ===================== Algorithmic Pseudocode =====================
\egin{algorithm}[H]
\caption{SAMS: Environment Construction and Government Policy Learning}
\label{alg:sams}
\begin{algorithmic}[1]
\State \textbf{Inputs:} Backbone macro model $\mathcal{M}$; type distribution $\mathcal{D}_{\text{types}}$; shock process $\mathcal{P}_{\varepsilon}$; welfare objective $W$; pricing/aggregation map $\mathcal{A}$; tolerances $\epsilon_{\text{Euler}}, \epsilon_{\text{GE}}$.
\State \textbf{Initialize:} Household NN $f_{\phi}$ (policy rules); government policy $\pi_{\theta}$; price vector or wedges $p$; replay buffer $\mathcal{B}\leftarrow\emptyset$.

\Statex
\Function{HouseholdSolver}{$p,\ \text{policy context}$}
  \Repeat
    \State Sample Monte Carlo batch of states $(x,g)\sim \mathcal{D}_{\text{types}}\times \mathcal{P}_{\varepsilon}$ and aggregate state $s$.
    \State Compute decisions $d=f_{\phi}(x,g,s)$.
    \State Evaluate Euler residuals $\mathcal{R}(d; x,g,s,p)$ from F.O.C.
    \State Update $\phi \leftarrow \phi - \eta_{\phi}\nabla_{\phi}\big[\mathbb{E}(\|\mathcal{R}\|^2)\big]$.
  \Until{$\mathbb{E}(\|\mathcal{R}\|^2) < \epsilon_{\text{Euler}}$}
  \State \textbf{return} $f_{\phi}$
\EndFunction

\Statex
\Repeat \Comment{\emph{Outer equilibrium / training loop}}
  \State $f_{\phi}\leftarrow$\Call{HouseholdSolver}{$p,\ \pi_{\theta}$} \Comment{Solve household optimization at current prices/policy}
  \State Simulate cross-section using $f_{\phi}$; aggregate via $\mathcal{A}$ to get new $(\tilde{p}, \tilde{s})$ trajectories.
  \If{$\| \tilde{p}-p \| > \epsilon_{\text{GE}}$}
    \State Update prices $p \leftarrow \lambda \tilde{p} + (1-\lambda)p$ \Comment{Damping for GE convergence}
  \EndIf

  \State \textbf{RL Rollouts:}
  \For{episodes $e=1,\dots,E$}
    \State Reset aggregate state $s_0$; sample micro states $(x^i_0,g^i_0)$ across households.
    \For{$t=0,1,\dots,T-1$}
      \State Government acts: $a_t \sim \pi_{\theta}(\cdot \mid s_t)$ \Comment{e.g., tax schedule parameters}
      \State Households respond via $f_{\phi}$; environment transitions: $(s_{t+1}, \{x^i_{t+1},g^i_{t+1}\})$.
      \State Compute reward $r_t = W(s_t,a_t)$ (e.g., social welfare with weights/penalties).
      \State Store $(s_t,a_t,r_t,s_{t+1})$ in buffer $\mathcal{B}$.
    \EndFor
  \EndFor

  \State \textbf{RL Update:} $\theta \leftarrow \text{UpdatePolicy}(\theta;\mathcal{B})$ \Comment{e.g., PPO/SAC with baselines}
  \State Optionally clear/decay buffer $\mathcal{B}$; assess stopping criteria.
\Until{convergence of $(p,\phi,\theta)$ or budget exhausted}

\State \textbf{Output:} Trained household solver $f_{\phi^\star}$, equilibrium prices $p^\star$, and government policy $\pi_{\theta^\star}$.
\end{algorithmic}
\end{algorithm}


\subsection{Page Style}

The easiest way to fulfill the formatting requirements is to use this document as a \textit{template}. \textit{Author} can directly write on this \textit{template} file. The minimum article page length is 8 pages and the maximum is 15 pages, including all figures, tables, nomenclature, references, and others. Articles are written in A4 paper format with a right-left-bottom border of 2 cm and an upper border of 3.25 cm. The article format uses 1 column.

\section{RESULTS AND DISCUSSION}

Research results in the form of data or numbers are presented in tables or graphs. If the research is carried out application/software development, some important \textit{screenshots} can be presented. Each table, graph or figure must be referred to in the text/paragraph.

The discussion section provides \textit{insights} into the data obtained in the research. This section can present tables or graphs that are the result of data processing (not just raw data). The author is required to explain the findings obtained in the research accompanied by clear evidence. This section can contain reviews that compare the results obtained in this study with the results obtained in previous studies.

\noindent\textbf{1. Writing Format}

Paragraphs should be organized and consistent. Pay attention to the spelling and formatting of foreign terms. All paragraphs must be right-aligned and left-aligned. The entire document should be written in Times New Roman font with \textit{single} spacing. Other fonts may be used if there is a specific purpose. The title consists of a maximum of 10 words.

\noindent\textbf{2. \textit{Author}}

Authors should not indicate the name of their position (e.g. Supervisor), academic degree (e.g. Dr) or membership of any professional organization (e.g. Senior Member IEEE). To avoid confusion, the last name of each \textit{author} should be written at the end, not abbreviated and marked with a comma (e.g. Rizky Prabowo becomes Prabowo, Rizky). Each affiliation must include, at a minimum, the name of the company and the name of the author's country of residence (e.g. University of Lampung, Indonesia). An email address is required for the corresponding author.

\noindent\textbf{3. Math Formulation}

Equations must be written using the latest version of Ms Equation Editor (found in the latest version of Ms Word) or using the Mathtype application. Writing symbol descriptions in equations is made in descriptive paragraphs, not list items as in book writing. Equations must be typed with an indent of 1.27 pt and numbered sequentially starting with the number (1) on the right. \textit{Author} can use \textit{ref\_rumus style} for formula writing rules.

\begin{equation}
    \label{eq:formula}
    \int_{0}^{\infty} e^{-x^2} dx = \frac{\sqrt{\pi}}{2} \hspace{\eqmargin} (1)
\end{equation}

\noindent\textbf{4. Table and Figure/Graphic Writing}

All figures and tables must be centered and numbered consecutively. All text within figures and tables must be clearly legible, no \textit{blurring}. Each figure and table must be referred to in the text, the way of referring should not use location (e.g. below, above, following, etc.). Writing tables, figures, graphs, and equations must have high resolution (min 300 dpi). Avoid cropping images or \textit{screenshots} for writing tables and equations.

\begin{pepadunfigure}{Unila logo}
    \includegraphics[width=0.3\textwidth]{unila-logo.png} % Ganti dengan file logo
\end{pepadunfigure}

The \textit{author} can use the image \textit{style} for image writing rules while for table writing, the \textit{author} can use the Table \textit{style}. Tables are made with horizontal lines without using vertical lines. Tables must not be cut off on other pages.

\begin{pepaduntable}{Summary of physical parameters}
    \begin{tabular}{@{}c c c c@{}}
        \toprule
        \textbf{No.} & \textbf{Segments} & \textbf{Length (km)} & \textbf{Elevation (meters)} \\
        \midrule
        1 & A-B & 25 & 30 \\
        2 & B-C & 75.15 & 10 \\
        3 & C-D & 44.75 & 50 \\
        4 & D-E & 72.5 & 10 \\
        5 & E-F & 21.25 & 10 \\
        \bottomrule
    \end{tabular}
\end{pepaduntable}

\noindent\textbf{5. Bibliography Writing}

References in the text must be cited by writing the author's last name and its sequence number in the reference. The citation style used in Jurnal Pepadun is IEEE \cite{Zhang2023}. Bibliography writing also uses IEEE style. Citations are sorted by their appearance in the text and use Arabic numerals in square brackets \cite{Sugumar2025}. It is highly recommended to use citation management applications such as Mendeley or Reference Manager when writing articles \cite{Nie2021,Drogkoula2023,Jiang2023} so that citation management becomes easy. Writing a list of references using single \textit{spacing} using \textit{font size 11pt, spacing before 0 after 6pt}.

\section{CONCLUSIONS}

The Conclusion section summarizes the reviews written in the Results and Discussion sections. The Conclusion section is written in paragraph form, no need to use numbers. The paper will not be reformatted, so please stick to the instructions given above. Otherwise, it will be returned to the \textit{author}. Please upload your article in .doc/.docx form on the Jurnal Pepadun website \url{https://pepadun.fmipa.unila.ac.id}. Articles sent via e-mail will not be processed.

\begin{nomenclature}
    $A$      & = & Amplitude \\
    $C_d$    & = & drag coefficient \\
    $f_e$    & = & linearization coefficient \\
    $K_i$    & = & modification factor \\
\end{nomenclature}

\begin{acknowledgments}
If necessary, acknowledgments can be made here. Acknowledgments should be directed to research funders or experts who provided significant assistance in the completion of the research.
\end{acknowledgments}

% Daftar Pustaka
\printbibliography[title={REFERENCES }, heading=bibintoc]  % Gunakan judul LITERATURE

\end{document}