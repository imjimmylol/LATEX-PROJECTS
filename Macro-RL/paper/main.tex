% main.tex
\documentclass[11pt]{article}
\usepackage{pepadun}

\usepackage[backend=biber, style=ieee]{biblatex}
\addbibresource{pepadun.bib}  % Gunakan file bib yang disediakan

% Metadata dokumen
\title{Semi-RL Autonomous Macro System: A Hybrid Approach for Reinforcement Learning in Macroeconomics}
\author{%
    \begin{tabular}{c}
        \textsuperscript{1}Jing-Ho, Chen, \\
        \textsuperscript{2}Next Author and \\[6pt]
        \textsuperscript{1,2}Economic, National Chengchi University, \\
        No.64, Sec.2, ZhiNan Rd., Wenshan District, Taipei City 116011, Taiwan (R.O.C.) \\[6pt]
        e-mail: \textsuperscript{1} zhingehe78@gmail.com, \\
    \end{tabular}%
}
\date{} % Hapus tanggal otomatis

\begin{document}

\maketitle

\begin{abstract}
TODO
\end{abstract}

\keywords{RL, DL, ABM}

\section{INTRODUCTION}

Reinforcement learning (RL) has recently been explored as a tool for macroeconomic modeling, but current approaches fall into two broad categories, each with notable limitations. (1) Single-agent RL with a representative agent: Here a single RL "agent" represents an entire sector (e.g. the representative household or firm) interacting with an abstract macroeconomic environment. This approach simplifies the economy but often requires very restrictive assumptions. It struggles to handle complex lifetime utility maximization with expectations, so researchers are forced to use overly simplified representative agents.

Moreover, because most of the economy is baked into the fixed environment dynamics, these models lack rich agent dynamics – making it difficult to capture how policy changes propagate over time through agent behavior. (2) Multi-agent RL with many agents: In this approach, multiple agents (households, firms, government, etc.) are each modeled as RL agents. While this allows heterogeneity and potentially richer interactions, multi-agent RL systems are notoriously hard to train. They often suffer from instability and have no guarantee of converging to a stable equilibrium as the number of agents grows
jmlr.org
. This instability severely limits scalability. Yet, to study crucial macro questions like income inequality or optimal taxation, we need to simulate a large population of heterogeneous agents – something that neither single-agent nor naive multi-agent RL handles well under current methods.

To address these challenges, we propose a new framework called the Semi-RL Autonomous Macro System (SAMS). This hybrid approach combines the strengths of traditional economic modeling with modern RL and deep learning techniques. The key idea is to let the majority of agents in the model behave according to established economic principles (following their first-order conditions, or F.O.C., for optimality) while reserving RL for the elements of the system that we want to learn or optimize (such as a policy-making agent). In other words, instead of making every agent an RL learner (which causes learning instability) or reducing the whole economy to one trivial RL agent (which oversimplifies dynamics), we split the work: non-RL agents follow adaptive decision rules derived from economic theory, and a limited number of RL agents learn optimal policies within this environment. The non-RL agents are not static; they behave autonomously by optimizing their objectives (e.g. utility or profit) via their F.O.C., which means they respond realistically to changes in the economic environment. This creates a dynamic and responsive macroeconomic simulator, but without the explosion of learning complexity that plagues full MARL systems. 

Technically, our framework leverages a Monte Carlo simulation-based deep learning method to solve these agents’ optimization problems. Recent advances in deep learning have shown that we can efficiently solve high-dimensional dynamic models (such as heterogeneous-agent macro models) by casting them as neural-network approximations to Bellman or Euler equations
web.stanford.edu
. In particular, we build on the method of Maliar et al. (2021), who demonstrated a deep learning approach capable of handling large-scale heterogeneous-agent models (e.g. the Krusell–Smith model with thousands of agents) without resorting to the usual simplifying assumptions
web.stanford.edu
. By incorporating this deep learning Monte Carlo solver, SAMS frees us from having to assume a single representative agent. We can include a rich variety of heterogeneous agents whose decision rules are computed through the deep learning framework, ensuring that each agent (or type of agent) optimally responds to its economic incentives and constraints. These agents effectively simulate the rational behavior one would expect in a macroeconomic model, but with computational tractability even in large numbers.

Meanwhile, the elements of the system that do use RL (for example, a policy agent like a government adjusting tax rates) can learn and adapt their strategies over time. Because the bulk of the economy (households, firms, etc.) is handled by the deep-learning-based F.O.C. solvers, the overall simulation remains much more stable. The environment that the RL agent interacts with is grounded in equilibrium behavior (through the non-RL agents), which is less non-stationary than an all-RL multi-agent system. As a result, SAMS can scale to a large number of heterogeneous agents without suffering the divergence or chaos that typically occurs in multi-agent RL. This hybrid design significantly improves stability and realism: policy effects can be observed through the adaptive responses of many simulated agents, yet the training process remains feasible. 

In summary, the Semi-RL Autonomous Macro System offers a balanced approach that bridges the gap between existing RL-in-macro methods. It retains the rich dynamics and heterogeneity of agent-based models while constraining the learning problem to a manageable scope. By focusing RL where it is most needed (e.g. learning optimal policy) and using deep learning to handle traditional agent optimization, this framework aims to enable robust macroeconomic simulations for scenarios like inequality, taxation, and beyond. We anticipate that SAMS can unlock more realistic and scalable macroeconomic policy analysis, combining the strengths of reinforcement learning with the rigor of dynamic economic modeling.


\section{RESEARCH METHODOLOGY}

We develop a hybrid framework that combines deep-learning solutions to dynamic economic optimization with a targeted reinforcement learning (RL) policy agent. RL is reserved solely for the government, while households (and firms, if included) are modeled as optimizing agents whose decision rules are computed via a neural network (NN) solver that minimizes Euler-equation residuals. This preserves equilibrium discipline and heterogeneity without incurring the instability typical of full multi-agent RL.

\paragraph{Stage 1: Economic environment construction.}
We specify a backbone macroeconomic model with heterogeneous households, preference and technology parameters, market structure, and aggregate shocks. Each household type’s policy function is represented by a neural network $f_{\phi}$ that maps states (idiosyncratic and aggregate) into decisions (e.g., consumption, savings, labor). Rather than learning from demonstrations, $f_{\phi}$ is trained by minimizing Euler residuals implied by agents’ first-order conditions, following \cite{maliar2021deep}. This is akin to an imitation-learning analogue in which the ``expert'' is the model’s optimality conditions rather than data \cite{ho2016generative}. Monte Carlo simulation over the joint state distribution enables scalable training across high-dimensional state spaces and rich heterogeneity.

\paragraph{Stage 2: Equilibrium-consistent environment dynamics.}
Given $f_{\phi}$, we simulate the cross-section of households and aggregate to market-level objects (e.g., factor supplies, demands). Prices or wedges are updated by an aggregator (e.g., market-clearing or reduced-form pricing map), delivering a dynamic, equilibrium-consistent environment that responds to policy changes. We iterate between (i) updating $f_{\phi}$ by minimizing Euler residuals under current prices/policy and (ii) recomputing aggregates until residuals and aggregates stabilize within tolerance.

\paragraph{Stage 3: RL-based policy optimization.}
With a responsive, equilibrium-grounded environment in place, we introduce the government RL agent. 
The complete procedure is summarized in Algorithm~\ref{alg:sams} (see Appendix).

\medskip
\noindent
Overall, the system alternates between (i) solving households’ optimization via Euler-residual minimization and (ii) improving the government’s policy via RL, yielding a Semi-RL Autonomous Macro System that is both scalable and theoretically disciplined.


\subsection{Deriving environment dynamics and Learning objectives (stage1 and stage2)}

We choose the structure of Bewley-Aiyagari model as our Macro backbone following the article \cite{mi2023taxai}

\paragraph{2.1.1} N Households

The households faces the following obtimization problem :
\begin{equation}
\begin{aligned}
\max_{\{c_t,h_t,a_{t+1}\}_{t=0}^{T_N}} \quad 
& \mathbb{E}_0 \sum_{t=0}^{T_N} \beta^t 
\left( \frac{c_t^{1-\theta}}{1-\theta} - \frac{h_t^{1+\gamma}}{1+\gamma} \right) \\[4pt]
\text{s.t.}\quad 
& (1+\tau_s)c_t + a_{t+1} = i_t - T(i_t) + a_t - T^{a}(a_t), \\
& m_t \coloneqq i_t - T(i_t) + a_t - T^{a}(a_t),\\
& i_t = w_t e_t h_t + r_{t-1} a_t,\\
& a_{t+1} \ge 0 \iff (1+\tau_s)c_t \le m_t \iff c_t \le \frac{m_t}{1+\tau_s}.
\end{aligned}
\tag{1}
\end{equation}

Then by the method proposed in \cite{maliar2021deep} 
we can solve this problem using deep learning by the objective function 
of minimizing the Euler-residuals as follows\ref{objfunc:AiO}.(see appendix B) 
\begin{equation}
\begin{aligned}
\mathbb{E}_{\vec{\omega}} \Big\{ &
    \left[\,
      \phi^{\text{FB}}\!\left(
        \tfrac{a_{t+1}}{m_t},\,
        1 - h_{\phi}
      \right)
    \right]^2
    \\
    &\quad +\;
    \upsilon_h \Bigg(
      \,h - 
      \frac{U'(c_{t+1},h_{t+1})|_{\vec{\epsilon}=\vec{\epsilon_1}}}
           {U'(c_t,h_t)}
      \Big\{ r_t - T'(i_{t+1}) r_t + 1 - {T^a}'(a_{t+1}) \Big\}
    \Bigg)
    \\
    &\qquad \times
    \Bigg(
      h - 
      \frac{U'(c_{t+1},h_{t+1})|_{\vec{\epsilon}=\vec{\epsilon_2}}}
           {U'(c_t,h_t)}
      \Big\{ r_t - T'(i_{t+1}) r_t + 1 - {T^a}'(a_{t+1}) \Big\}
    \Bigg)
\;\Big\}.
\end{aligned}\tag{2}
\end{equation}
\subsection{RL-based policy optimization}




\appendix

\section{Algorithmic Details}
\begin{algorithm}[H]
\caption{SAMS: Environment Construction and Government Policy Learning}
\label{alg:sams}
\begin{algorithmic}[1]
\State \textbf{Inputs:} Backbone macro model $\mathcal{M}$; type distribution $\mathcal{D}_{\text{types}}$; shock process $\mathcal{P}_{\varepsilon}$; welfare objective $W$; pricing/aggregation map $\mathcal{A}$; tolerances $\epsilon_{\text{Euler}}, \epsilon_{\text{GE}}$.
\State \textbf{Initialize:} Household NN $f_{\phi}$ (policy rules); government policy $\pi_{\theta}$; price vector or wedges $p$; replay buffer $\mathcal{B}\leftarrow\emptyset$.

\Statex
\Function{HouseholdSolver}{$p,\ \text{policy context}$}
  \Repeat
    \State Sample Monte Carlo batch of states $(x,g)\sim \mathcal{D}_{\text{types}}\times \mathcal{P}_{\varepsilon}$ and aggregate state $s$.
    \State Compute decisions $d=f_{\phi}(x,g,s)$.
    \State Evaluate Euler residuals $\mathcal{R}(d; x,g,s,p)$ from F.O.C.
    \State Update $\phi \leftarrow \phi - \eta_{\phi}\nabla_{\phi}\big[\mathbb{E}(\|\mathcal{R}\|^2)\big]$.
  \Until{$\mathbb{E}(\|\mathcal{R}\|^2) < \epsilon_{\text{Euler}}$}
  \State \textbf{return} $f_{\phi}$
\EndFunction

\Statex
\Repeat \Comment{\emph{Outer equilibrium / training loop}}
  \State $f_{\phi}\leftarrow$\Call{HouseholdSolver}{$p,\ \pi_{\theta}$} \Comment{Solve household optimization at current prices/policy}
  \State Simulate cross-section using $f_{\phi}$; aggregate via $\mathcal{A}$ to get new $(\tilde{p}, \tilde{s})$ trajectories.
  \If{$\| \tilde{p}-p \| > \epsilon_{\text{GE}}$}
    \State Update prices $p \leftarrow \lambda \tilde{p} + (1-\lambda)p$ \Comment{Damping for GE convergence}
  \EndIf

  \State \textbf{RL Rollouts:}
  \For{episodes $e=1,\dots,E$}
    \State Reset aggregate state $s_0$; sample micro states $(x^i_0,g^i_0)$ across households.
    \For{$t=0,1,\dots,T-1$}
      \State Government acts: $a_t \sim \pi_{\theta}(\cdot \mid s_t)$ \Comment{e.g., tax schedule parameters}
      \State Households respond via $f_{\phi}$; environment transitions: $(s_{t+1}, \{x^i_{t+1},g^i_{t+1}\})$.
      \State Compute reward $r_t = W(s_t,a_t)$ (e.g., social welfare with weights/penalties).
      \State Store $(s_t,a_t,r_t,s_{t+1})$ in buffer $\mathcal{B}$.
    \EndFor
  \EndFor

  \State \textbf{RL Update:} $\theta \leftarrow \text{UpdatePolicy}(\theta;\mathcal{B})$ \Comment{e.g., PPO/SAC with baselines}
  \State Optionally clear/decay buffer $\mathcal{B}$; assess stopping criteria.
\Until{convergence of $(p,\phi,\theta)$ or budget exhausted}

\State \textbf{Output:} Trained household solver $f_{\phi^\star}$, equilibrium prices $p^\star$, and government policy $\pi_{\theta^\star}$.
\end{algorithmic}
\end{algorithm}



\section{environment construction}
\subsection{Household problem : Euler Equation and KKT}

We begin by setting up the Lagrangian of the representative household’s optimization problem:
\begin{equation}
\begin{aligned}
L &= \mathbb{E}_0 \sum_{t=0}^{T_N} \beta^t \Big\{ U(c_t,h_t) 
+ \lambda_t \big[ w_t e_t h_t + r_{t-1} a_t - T(w_t e_t h_t + r_{t-1} a_t) \\
&\qquad + a_t - T^a(a_t) - a_{t+1} - (1+\tau_c)c_t \big] 
+ \mu_t a_{t+1} \Big\}.
\end{aligned}
\end{equation}

\paragraph{First-order condition with respect to consumption.}
\begin{equation}
[c_t]: \quad \mathbb{E}_0 \big[ U_c(c_t,h_t) \big] 
= \lambda_t (1+\tau_s).
\end{equation}

\paragraph{First-order condition with respect to savings.}
\begin{equation}
\begin{aligned}
[a_{t+1}]:\quad &
\mathbb{E}_0 \!\left[ \beta^t \{-\lambda_t + \mu_t\} 
+ \beta^{t+1} \big\{ \lambda_{t+1}\,[\,r_t - T'(i_{t+1})r_t + 1 - {T^a}'(a_{t+1})\,] \big\} \right] = 0 \\
&= \mathbb{E}_0 \!\left[ -\lambda_t + \mu_t 
+ \beta \,\lambda_{t+1}\,[\,r_t - T'(i_{t+1})r_t + 1 - {T^a}'(a_{t+1})\,] \right] = 0 \\
&\implies \mathbb{E}_0[\mu_t] 
= \mathbb{E}_0 \beta \!\left[ \lambda_t 
- \lambda_{t+1}\,[\,r_t - T'(i_{t+1})r_t + 1 - {T^a}'(a_{t+1})\,] \right].
\end{aligned}
\end{equation}

\paragraph{First-order condition with respect to labor supply.}
\begin{equation}
\begin{aligned}
[h_t]: \quad &
\mathbb{E}_0 \Big[ \beta^t \big( U_h(c_t,h_t) + \lambda_t[\,w_t e_t - T'(i_t)\,] \big) \Big] = 0 \\
&\iff \mathbb{E}_0 \Big[ -h_t^{-\gamma} 
+ \lambda_t \big( w_t e_t 
- (w_t e_t h_t + r_{t-1}a_t 
- (1-\tau)\tfrac{[\,w_t e_t h_t + r_{t-1}a_t\,]^{1-\xi}}{1-\xi}) \big) \Big] = 0 \\
&\iff \mathbb{E}_0 \Big[ -h_t^{-\gamma} 
+ \frac{U_c(c_t,h_t)}{1+\tau_s}\big( w_t e_t 
- (w_t e_t h_t + r_{t-1}a_t 
- (1-\tau)\tfrac{[\,w_t e_t h_t + r_{t-1}a_t\,]^{1-\xi}}{1-\xi}) \big) \Big] = 0
\end{aligned}
\end{equation}

\paragraph{Euler equation for consumption and savings.}
Combining (B.2) and (B.3), we obtain the intertemporal Euler equation:
\begin{equation}
\mu_t 
= \frac{U_c(c_t,h_t)}{1+\tau_s}
- \frac{\mathbb{E}_0 \, \beta U_c(c_{t+1},h_{t+1})}{1+\tau_s}
\Big\{ r_t - T'(i_{t+1})r_t + 1 - {T^a}'(a_{t+1}) \Big\}.
\end{equation}

Finally, By KKT conditiosn : $\mu_t>0$, $a_{t+1}>0$, and $\mu a_{t+1}=0$, applying the Fischer-Burmeister (FB) function $\psi^{\text{FB}}(x, y) = x+y-\sqrt{x^2+y^2} $ and turning into unit-free form 

\begin{equation}
x \coloneqq \frac{a_{t+1}}{m_t}\in [\frac{\underline{a}}{m_t}, 1], 
\qquad \underline{a} > 0.
\end{equation}

\begin{equation}
y \coloneqq 
1 - \frac{\mathbb{E}_0 \beta U'(c_{t+1},h_{t+1})}{U'(c_t,h_t)}
\Big\{\, r_t - T'(i_{t+1})\,r_t + 1 - {T^a}'(a_{t+1}) \,\Big\}.
\end{equation}

\pagebreak

\subsection{Household problem : AiO Expectation form and reparameterization for functoianl approximation}
There are two sources of uncertainty in the households problem namely intial condition and income shock.
So we define a set of random varbles according to its source $\vec{\omega}=(\vec{\iota }, \vec{\varepsilon })$
Plug B.6 and B.7 into the Fischer-Burmeister fucntion
\begin{equation}
\mathbb{E}_{\vec{\iota}} \left[
\phi^{\text{FB}}\!\left(
\frac{a_{t+1}}{m_t},\,
1 - \frac{\mathbb{E}_{\epsilon} \beta U'(c_{t+1},h_{t+1})}{U'(c_t,h_t)}\Big\{\, r_t - T'(i_{t+1})\,r_t + 1 - {T^a}'(a_{t+1}) \,\Big\}
\right)
\right]^{2}
\end{equation}

We use a neural network to approximate household behavior,
$f_\theta(\iota;\theta) \approx \frac{a_{t+1}}{m_t}$. Furthermore, to
facilitate Monte Carlo simulation, following \cite{FiaccoMcCormick1968},
we introduce an auxiliary variable $\mu_\phi(\iota;\phi) \approx \frac{\mathbb{E}_{\epsilon} U'(c_{t+1},h_{t+1})}{U'(c_t,h_t)}\Big\{\, r_t - T'(i_{t+1})\,r_t + 1 - {T^a}'(a_{t+1}) \,\Big\}$ and add the
penalty term $\upsilon_{\mu}\!\left(\mu - b(\cdot)\right)$. Setting $\upsilon_{\mu}$
large (or annealing it upward) enforces the equality constraint in the
exact-penalty sense \cite{nocedal2006numerical}. As a result, we use (B.9) to approximate (B.8)


\begin{equation}
  \mathbb{E}_{\vec{\iota}}\left[\phi^{\text{FB}}\!\left(
  \frac{a_{t+1}}{m_t},\,
  1 - \mu_{\phi}
  \right) \right]^2 + 
  \underbrace{\left[ \upsilon_{\mu}\!\left(\mu_{\phi} - \frac{\mathbb{E}_{\vec{\epsilon}} \beta U'(c_{t+1},h_{t+1})}{U'(c_t,h_t)}\Big\{\, r_t - T'(i_{t+1})\,r_t + 1 - {T^a}'(a_{t+1}) \,\Big\}\right) \right]^2}_{\text{penalty factor}}
\end{equation}

By applying the AiO operator proposed in \cite{maliar2021deep} we can rewrite (B.9) as follows
\label{objfunc:AiO}
\begin{equation}
\begin{aligned}
\mathbb{E}_{\vec{\omega}} \Big\{ &
    \left[\,
      \phi^{\text{FB}}\!\left(
        \tfrac{a_{t+1}}{m_t},\,
        1 - \mu_{\phi}
      \right)
    \right]^2
    \\
    &\quad +\;
    \upsilon_h \Bigg(
      \,\mu - 
      \frac{\beta U'(c_{t+1},h_{t+1})|_{\vec{\epsilon}=\vec{\epsilon_1}}}
           {U'(c_t,h_t)}
      \Big\{ r_t - T'(i_{t+1}) r_t + 1 - {T^a}'(a_{t+1}) \Big\}
    \Bigg)
    \\
    &\qquad \times
    \Bigg(
      \mu - 
      \frac{\beta U'(c_{t+1},h_{t+1})|_{\vec{\epsilon}=\vec{\epsilon_2}}}
           {U'(c_t,h_t)}
      \Big\{ r_t - T'(i_{t+1}) r_t + 1 - {T^a}'(a_{t+1}) \Big\}
    \Bigg)
\;\Big\}.
\end{aligned}
\end{equation}


\section{Coding Details}

In this section, we provide specific coding details for implementing the Semi-RL Autonomous Macro System (SAMS) framework. The implementation is done using Python with libraries such as TensorFlow/PyTorch for deep learning and Stable Baselines3 for reinforcement learning.

\subsection{Environment Construction : Household Solver Implementation}

We need to calculate the upper bound for the whole economy to restrict and normalize the action space of households and government.

First we pin down price bounds from firm FOCs and the no-arbitrage condition, by the firm's first-order conditions (FOCs) for profit maximization, we have:
\begin{equation}
\begin{aligned}
\frac{\partial Y_t}{\partial L_t} &= 0 \implies W_t = (1 - \alpha) \left( \frac{K_t}{L_t} \right)^{\alpha} \\
\frac{\partial Y_t}{\partial K_t} &= 0 \implies R_t = \alpha \left( \frac{K_t}{L_t} \right)^{\alpha - 1}
\end{aligned}
\end{equation}

Then, apply the no-arbitrage condition from financial intermediary $R_{t+1} = r_t + \delta$, since $r_t \in [0,1]$, we have 
\begin{equation}
  R_{t} = \alpha \left( \frac{K_t}{L_t} \right)^{\alpha - 1} \le 1+\delta, 
  \implies \alpha \left( \frac{K_t}{L_t} \right)^{\alpha-1} \le 1+\delta 
  \implies \frac{K_t}{L_t} \le \left(\frac{1+\delta}{\alpha}\right)^{\frac{1}{\alpha-1}}.
\end{equation}
Plug (C.2) back into (C.1) we have the upper bound of wage rate :
\[
 w_t = (1 - \alpha) \left( \frac{K_t}{L_t} \right)^{\alpha} \leq \left( \frac{1+\delta}{\alpha} \right)^{\frac{\alpha}{\alpha-1}} \coloneqq \bar{W^*}.
\]

Secondly, consider the household's income process $i_t = w_t e_t h_t + r_{t-1} a_t$, since given $\rho<1$ and $h_t \leq 1$ and clearly $e_t \in [0, e^{*}]$ is bounded, provided 
$\bar{e^*} = \text{Max} \left[v_{T_0+1} = v_{0}^{(T_0+1)\rho_v} 
\cdot \exp\!\left(\sigma_v \sum_{s=0}^{T_0} \rho_v^{\,s} u_{t+s}\right), 
C_{\text{super}}^{T_0}v_{0}^{\rho_v} \exp\!\left(\sigma_v u_{t}\right)\right] $ since:

\paragraph{Upper bound for the normal state}
suppose the shock follows the AR(1) process without getting in the 
superstar state, then we can calculate the upper bound as follows :

\begin{equation*}
\begin{aligned}
    t = 0: \quad & v_{1} = v_{0}^{\rho_v} \exp\!\left(\sigma_v u_{t}\right), \\
    t = 1: \quad & v_{2} = \left(v_{0}^{\rho_v} \exp\!\left(\sigma_v u_{t}\right)\right)^{\rho_v} 
                    \cdot \exp\!\left(\sigma_v u_{t+1}\right), \\
    & \vdots \\
    t = T_0: \quad & v_{T_0+1} = v_{0}^{(T_0+1)\rho_v} 
      \cdot \exp\!\left(\sigma_v \sum_{s=0}^{T_0} \rho_v^{\,s} u_{t+s}\right).
\end{aligned}
\end{equation*}

\paragraph{Upper bound if remaining at superstar state}
suppose the shock remains at the superstar state for $T_0$ periods, then we can calculate the upper bound as follows :

\begin{equation*}
  \begin{aligned}
    t = 0: \quad & v_{1} = v_{0}^{\rho_v} \exp\!\left(\sigma_v u_{t}\right), \\
    t = 1: \quad & v_{2} \leq C_{\text{super}}v_{0}^{\rho_v} \exp\!\left(\sigma_v u_{t}\right), \\
    & \vdots \\
    t = T_0: \quad & v_{T_0+1} \leq C_{\text{super}}^{T_0}v_{0}^{\rho_v} \exp\!\left(\sigma_v u_{t}\right)
  \end{aligned}
\end{equation*}

Then by definition of money on hand $m_t \coloneqq i_t - T(i_t) + a_t - T^{a}(a_t)$, and replace each term with its upper bound, 
we can obtain $m_t \leq \bar{W^*} \bar{e^*} + \left(1+\bar{R}\right) \bar{a^*}$, 
and $\bar{a^*}$ can be calculated recursively since the propotional action satisfies the relation : 
$a_{t+1} \leq i_t + a_t$ given the intial conditions $\bar{e}$ and $\bar{a}$ and $\bar{e^*}, \bar{W^*}, \bar{h^*}$ and $\bar{R^*}$ which comes from the government action space. Note that these bounds are caluclated recursively using recursion. 

However, there are other ways to noramlize the environment, for example batch noramlization. That is calculate the upper bound and lower bound adaptivly according to each draws of shock and inital conditions. Experiments shown that this method is more robust and flexable to non-stationary economic status.

\pagebreak

\subsection{Neural Network Design}
We describe the design choices made for the neural network architectures used in our experiments. 
Our modeling framework involves two main components: the household policy network and the government policy network.

\subsubsection{Household policy network.}
We approximate the household policy function with a residual MLP augmented by Feature-wise Linear Modulation (FiLM). 
The conditioning signal is low-dimensional (policy instruments such as consumption, income, and wealth taxes), and FiLM efficiently injects this information by per-feature scaling and shifting \cite{Perez2018FiLM}. 
Unlike multi-head attention, which has quadratic complexity in sequence length, FiLM is computationally lightweight and parameter-efficient—an important advantage for large-scale simulations with many heterogeneous households. 

\subsubsection{Household Timing and Recursive System}

\paragraph{Within–period timing ($t \to t+1$).}
Note that the term "states" in this section simply refers to the equation solving process, it is different from the "state" in RL terminology.

\begin{enumerate}
    \item \textbf{State Realization (Beginning of Period $t$):}
    The state of the economy is given by the joint distribution of individual assets and productivities, $\{a_t^i, e_t^i\}_{i=1}^N$. The aggregate capital stock, $K_t$, is a \textbf{predetermined state variable} calculated from the sum of individual assets:
    $$
    K_t = \sum_{i=1}^{N} a_t^i
    $$

    \item \textbf{Factor Price Determination (Within Period $t$):}
    The wage $w_t$ and aggregate labor supply $L_t$ are determined \textbf{simultaneously} in a general equilibrium. Households take $w_t$ as given, but their collective actions must be consistent with it. The rental rate paid on assets from the previous period, $r_{t-1}$, is also realized.

    \item \textbf{Household Action (Within Period $t$):}
    Taking the equilibrium prices $(w_t, r_{t-1})$ as given, each household $i$ observes its individual state $(a_t^i, e_t^i)$ and chooses its control variables, labor supply $h_t^i$ and next-period assets $a_{t+1}^i$, according to its policy function $\pi_\theta$:
    $$
    (h_t^i, a_{t+1}^i) = \pi_\theta(a_t^i, e_t^i, \text{aggregate state})
    $$
    Consumption, $c_t^i$, is then determined as a residual from the budget constraint.

    \item \textbf{Flows and Aggregation:}
    Individual income $i_t^i$, disposable resources $m_t^i$, and consumption $c_t^i$ are realized based on the choices from Step 3.
    $$
    i_t^i = w_t e_t^i h_t^i + r_{t-1} a_t^i
    $$
    $$
    m_t^i = i_t^i - T(i_t^i) + a_t^i - T^{a}(a_t^i)
    $$
    $$
    c_t^i = \frac{m_t^i - a_{t+1}^i}{1+\tau_s}
    $$
    The aggregate capital for the next period is then determined by summing the individual savings choices:
    $$
    K_{t+1} = \sum_{i=1}^{N} a_{t+1}^i
    $$

    \item \textbf{State Transition ($t \to t+1$):}
    Idiosyncratic productivity shocks transition according to their stochastic process, $e_{t+1}^i \sim \Pi_e(\cdot | e_t^i)$. The new state of the economy for period $t+1$ is the set $\{a_{t+1}^i, e_{t+1}^i\}_{i=1}^N$, and the cycle repeats.
\end{enumerate}

\begin{enumerate}
  \item \textbf{States given : $\mathbf{\mathcal{S}_t}=\big( \mathbf{S}_{idv}, \mathbf{S}_{other}, \mathbf{S}_{pc} \big)$} \\
   Individual states $\mathbf{S}_{idv} = \big(a_t^i,v_t^i\big)$, the agent knows his own productivity and the asset holdings from previous periods.\\
   States of other agents $\mathbf{S}_{other} = \big( a_t^1, a_t^2, \ldots, a_t^N, v_t^1, v_t^2, \ldots, v_t^N \big)$, this network is a simple households equation based learning, so we assume a Gods view for the global states.\\
   Aggregate shocks/policy $\mathbf{S}_{pc} = \big( \tau_t^a, \xi_t^a ,\tau_t, \xi_t \big)$, the government policy instruments, in step3, it is depending on the actions of the government, which schedule will be learned via reinforcement learning.
  \item \textbf{Prices computed from aggregates : $\mathcal{P}(\mathcal{S}_t) = (w_t,r_{t})$} \\
  The prices are computed from the aggregates of all agents. 
  Note that the prices are calculated via the F.O.C. of the firm, and we assume all the savings are invested in the capital market, and the capital follows the law of motion $K_{t+1} = (1-\delta)K_t + I_t$.
  \item \textbf{Household chooses controls:} $\pi_\theta(\mathbf{\mathcal{S}_t})=(a_{t+1}, h_t)$.
  \item \textbf{Flows and budget/ taxes realize:}
  \[
    i_t = w_t e_t h_t + r_{t-1} a_t,\qquad
    m_t \coloneqq i_t - T(i_t) + a_t - T^{a}(a_t),
  \]
  \[
    (1+\tau_s)\,c_t + a_{t+1} = m_t,\qquad c_t \le \frac{m_t}{1+\tau_s},\quad a_{t+1}\ge 0,\quad h_t\in[0,h_{\max}].
  \]
  \item \textbf{Idiosyncratic/aggregate transitions:}
  \[
    e_{t+1}\sim \Pi_e(\cdot\mid e_t,\mathbf{S}_t),\qquad
    \mathbf{S}_{t+1} \sim \Pi_S(\cdot\mid \mathbf{S}_t,\text{aggregates}).
  \]
\end{enumerate}

\paragraph{Vectors and objects.}
\[
\mathbf{S}_t \;=\; \big(z_t,\;\text{tax params }(\tau_s,\text{params of }T, T^a),\;\text{other aggregate states}\big),
\]
\[
\mathbf{s}_t \;=\; \big(a_t,\,e_t\big), \qquad
\mathbf{u}_t \;=\; \big(c_t,\,h_t,\,a_{t+1}\big).
\]

\paragraph{Preferences.}
\[
\max_{\{c_t,h_t,a_{t+1}\}} \; \mathbb{E}_0\sum_{t=0}^{T_N}\beta^t
\left( \frac{c_t^{1-\theta}}{1-\theta} - \frac{h_t^{1+\gamma}}{1+\gamma} \right).
\]

\paragraph{Prices (example Cobb–Douglas).}
If aggregates imply $K_t$ and $L_t=\sum_i e_t^{(i)}h_t^{(i)}$, then
\[
w_t=(1-\alpha)\Big(\tfrac{K_t}{L_t}\Big)^{\alpha},\qquad
r_{t-1}=\alpha\Big(\tfrac{K_{t-1}}{L_{t-1}}\Big)^{\alpha-1}.
\]

\paragraph{Feasibility and tax system.}
\[
(1+\tau_s)c_t + a_{t+1} = \underbrace{w_t e_t h_t + r_{t-1} a_t}_{i_t}
\;-\; T(i_t)\;+\; a_t \;-\; T^a(a_t),
\]
\[
c_t \in \Big[0,\;\tfrac{m_t}{1+\tau_s}\Big],\quad h_t\in[0,h_{\max}],\quad a_{t+1}\in[0,\infty).
\]

\paragraph{State transitions (Markov).}
\[
a_{t+1} \;=\; m_t - (1+\tau_s)c_t \;\equiv\; \mathcal{T}_a(\mathbf{s}_t,\mathbf{S}_t,\mathbf{u}_t),
\]
\[
e_{t+1} \sim \Pi_e(\cdot\mid e_t,\mathbf{S}_t),\qquad
\mathbf{S}_{t+1} \sim \Pi_S(\cdot\mid \mathbf{S}_t,\text{aggregates from }\{\mathbf{u}_t^{(i)}\}_i).
\]

\paragraph{Aggregation (for completeness).}
\[
K_{t+1}=\sum_i a_{t+1}^{(i)},\qquad L_t=\sum_i e_t^{(i)}h_t^{(i)},\qquad
\mathbf{S}_{t+1}=\mathcal{G}\big(\mathbf{S}_t,\{a_{t+1}^{(i)}\}_i,\{e_t^{(i)}h_t^{(i)}\}_i, \text{shocks}\big).
\]

\subsubsection{Household Solver (Neural Policy) — Inputs \& Outputs}

\paragraph{Solver input (network conditioning).}
\[
\text{NN input } \mathbf{x}_t \;=\; \Big[\; a_t,\; e_t,\; z_t,\; \text{tax params},\; \text{(optional price summaries)} \;\Big].
\]

\paragraph{Solver output (controls or parameterization).}
Two equivalent parameterizations are convenient:

\medskip
\noindent\textbf{(A) Direct controls}
\[
(c_t,\,h_t,\,a_{t+1}) \;=\; \pi_\theta(\mathbf{x}_t),
\quad \text{projected to } \mathcal{U}=\Big\{ c_t\!\in\!\big[0,\tfrac{m_t}{1+\tau_s}\big],\; h_t\!\in\![0,h_{\max}],\; a_{t+1}\!\ge\!0 \Big\}.
\]

\noindent\textbf{(B) Stable reparameterization}
\[
\big(\varsigma_t,\,h_t\big) = \pi_\theta(\mathbf{x}_t),
\qquad \varsigma_t \in (0,1) \text{ via sigmoid,}
\]
\[
c_t = \varsigma_t\cdot \frac{m_t}{1+\tau_s},\qquad
a_{t+1} = m_t - (1+\tau_s)c_t,
\]
which enforces the budget bound by construction.

\paragraph{Euler system (training target, schematic).}
\[
U_c(c_t,h_t) = \beta\,\mathbb{E}_t\!\left[ U_c(c_{t+1},h_{t+1})\,(1-\delta + r_t)\right],
\qquad
U_h(c_t,h_t) = -\,w_t e_t \, U_c(c_t,h_t),
\]
with $U_c(c,h)=c^{-\theta}$ and $U_h(c,h)=h^{\gamma}$, embedded as residuals (and tax wedges embedded in $m_t$ and prices).

\paragraph{Summary (markovian recursion).}
\[
(\mathbf{s}_{t+1},\mathbf{S}_{t+1})
\;=\;
\Big(\,\mathcal{T}_a(\mathbf{s}_t,\mathbf{S}_t,\pi_\theta(\mathbf{x}_t)),\;\Pi_e(\cdot),\;\Pi_S(\cdot)\,\Big).
\]

\subsubsection{Government Policy Network.}
[TODO]

\pagebreak

\printbibliography[title={REFERENCES }, heading=bibintoc]
\end{document}