\section{Appendix}

\subsection{Algorithmic Details}
\begin{algorithm}[H]
\caption{SAMS: Environment Construction and Government Policy Learning}
\label{alg:sams}
\begin{algorithmic}[1]
\State \textbf{Inputs:} Backbone macro model $\mathcal{M}$; type distribution $\mathcal{D}_{\text{types}}$; shock process $\mathcal{P}_{\varepsilon}$; welfare objective $W$; pricing/aggregation map $\mathcal{A}$; tolerances $\epsilon_{\text{Euler}}, \epsilon_{\text{GE}}$.
\State \textbf{Initialize:} Household NN $f_{\phi}$ (policy rules); government policy $\pi_{\theta}$; price vector or wedges $p$; replay buffer $\mathcal{B}\leftarrow\emptyset$.

\Statex
\Function{HouseholdSolver}{$p,\ \text{policy context}$}
  \Repeat
    \State Sample Monte Carlo batch of states $(x,g)\sim \mathcal{D}_{\text{types}}\times \mathcal{P}_{\varepsilon}$ and aggregate state $s$.
    \State Compute decisions $d=f_{\phi}(x,g,s)$.
    \State Evaluate Euler residuals $\mathcal{R}(d; x,g,s,p)$ from F.O.C.
    \State Update $\phi \leftarrow \phi - \eta_{\phi}\nabla_{\phi}\big[\mathbb{E}(\|\mathcal{R}\|^2)\big]$.
  \Until{$\mathbb{E}(\|\mathcal{R}\|^2) < \epsilon_{\text{Euler}}$}
  \State \textbf{return} $f_{\phi}$
\EndFunction

\Statex
\Repeat \Comment{\emph{Outer equilibrium / training loop}}
  \State $f_{\phi}\leftarrow$\Call{HouseholdSolver}{$p,\ \pi_{\theta}$} \Comment{Solve household optimization at current prices/policy}
  \State Simulate cross-section using $f_{\phi}$; aggregate via $\mathcal{A}$ to get new $(\tilde{p}, \tilde{s})$ trajectories.
  \If{$\| \tilde{p}-p \| > \epsilon_{\text{GE}}$}
    \State Update prices $p \leftarrow \lambda \tilde{p} + (1-\lambda)p$ \Comment{Damping for GE convergence}
  \EndIf

  \State \textbf{RL Rollouts:}
  \For{episodes $e=1,\dots,E$}
    \State Reset aggregate state $s_0$; sample micro states $(x^i_0,g^i_0)$ across households.
    \For{$t=0,1,\dots,T-1$}
      \State Government acts: $a_t \sim \pi_{\theta}(\cdot \mid s_t)$ \Comment{e.g., tax schedule parameters}
      \State Households respond via $f_{\phi}$; environment transitions: $(s_{t+1}, \{x^i_{t+1},g^i_{t+1}\})$.
      \State Compute reward $r_t = W(s_t,a_t)$ (e.g., social welfare with weights/penalties).
      \State Store $(s_t,a_t,r_t,s_{t+1})$ in buffer $\mathcal{B}$.
    \EndFor
  \EndFor

  \State \textbf{RL Update:} $\theta \leftarrow \text{UpdatePolicy}(\theta;\mathcal{B})$ \Comment{e.g., PPO/SAC with baselines}
  \State Optionally clear/decay buffer $\mathcal{B}$; assess stopping criteria.
\Until{convergence of $(p,\phi,\theta)$ or budget exhausted}

\State \textbf{Output:} Trained household solver $f_{\phi^\star}$, equilibrium prices $p^\star$, and government policy $\pi_{\theta^\star}$.
\end{algorithmic}
\end{algorithm}

\subsection{Environment Construction}
\subsubsection{Household Problem: Euler Equation and KKT}

We begin by setting up the Lagrangian of the representative householdâ€™s optimization problem:
\begin{equation}
\begin{aligned}
L &= \mathbb{E}_0 \sum_{t=0}^{T_N} \beta^t \Big\{ U(c_t,h_t) 
+ \lambda_t \big[ w_t e_t h_t + r_{t-1} a_t - T(w_t e_t h_t + r_{t-1} a_t) \\
&\qquad + a_t - T^a(a_t) - a_{t+1} - (1+\tau_c)c_t \big] 
+ \mu_t a_{t+1} \Big\}.
\end{aligned}
\end{equation}

\paragraph{First-order condition with respect to consumption.}
\begin{equation}
[c_t]: \quad \mathbb{E}_0 \big[ U_c(c_t,h_t) \big] 
= \lambda_t (1+\tau_s).
\end{equation}

\paragraph{First-order condition with respect to savings.}
\begin{equation}
\begin{aligned}
[a_{t+1}]:\quad &
\mathbb{E}_0 \!\left[ \beta^t \{-\lambda_t + \mu_t\} 
+ \beta^{t+1} \big\{ \lambda_{t+1}\,[\,r_t - T'(i_{t+1})r_t + 1 - {T^a}'(a_{t+1})\,] \big\} \right] = 0 \\
&= \mathbb{E}_0 \!\left[ -\lambda_t + \mu_t 
+ \beta \,\lambda_{t+1}\,[\,r_t - T'(i_{t+1})r_t + 1 - {T^a}'(a_{t+1})\,] \right] = 0 \\
&\implies \mathbb{E}_0[\mu_t] 
= \mathbb{E}_0 \beta \!\left[ \lambda_t 
- \lambda_{t+1}\,[\,r_t - T'(i_{t+1})r_t + 1 - {T^a}'(a_{t+1})\,] \right].
\end{aligned}
\end{equation}

\paragraph{First-order condition with respect to labor supply.}
\begin{equation}
\begin{aligned}
[h_t]: \quad &
\mathbb{E}_0 \Big[ \beta^t \big( U_h(c_t,h_t) + \lambda_t[\,w_t e_t - T'(i_t)\,] \big) \Big] = 0 \\
&\iff \mathbb{E}_0 \Big[ -h_t^{-\gamma} 
+ \lambda_t \big( w_t e_t 
- (w_t e_t h_t + r_{t-1}a_t 
- (1-\tau)\tfrac{[\,w_t e_t h_t + r_{t-1}a_t\,]^{1-\xi}}{1-\xi}) \big) \Big] = 0.
\end{aligned}
\end{equation}

\paragraph{Euler equation for consumption and savings.}
Combining the previous equations, we obtain the intertemporal Euler equation:
\begin{equation}
\mu_t 
= \frac{U_c(c_t,h_t)}{1+\tau_s}
- \frac{\mathbb{E}_0 \, \beta U_c(c_{t+1},h_{t+1})}{1+\tau_s}
\Big\{ r_t - T'(i_{t+1})r_t + 1 - {T^a}'(a_{t+1}) \Big\}.
\end{equation}

Finally, by KKT conditions: $\mu_t>0$, $a_{t+1}>0$, and $\mu a_{t+1}=0$, applying the Fischer-Burmeister (FB) function $\psi^{\text{FB}}(x, y) = x+y-\sqrt{x^2+y^2} $ and turning into unit-free form 

\begin{equation}
x \coloneqq \frac{a_{t+1}}{m_t}\in [\frac{\underline{a}}{m_t}, 1], 
\qquad \underline{a} > 0.
\end{equation}

\begin{equation}
y \coloneqq 
1 - \frac{\mathbb{E}_{\epsilon} \beta U'(c_{t+1},h_{t+1})}{U'(c_t,h_t)}
\Big\{\, r_t - T'(i_{t+1})\,r_t + 1 - {T^a}'(a_{t+1}) \,\Big\}.
\end{equation}

\subsubsection{Household Problem: AiO Expectation Form and Reparameterization for Functional Approximation}
There are two sources of uncertainty in the household's problem, namely initial condition and income shock.
So we define a set of random variables according to its source $\vec{\omega}=(\vec{\iota }, \vec{\varepsilon })$.
Plugging the previous equations into the Fischer-Burmeister function gives us:

\begin{equation}
\mathbb{E}_{\vec{\iota}} \left[
\phi^{\text{FB}}\!\left(
\frac{a_{t+1}}{m_t},\,
1 - \frac{\mathbb{E}_{\epsilon} \beta U'(c_{t+1},h_{t+1})}{U'(c_t,h_t)}\Big\{\, r_t - T'(i_{t+1})\,r_t + 1 - {T^a}'(a_{t+1}) \,\Big\}
\right)
\right]^{2}
\end{equation}

We use a neural network to approximate household behavior,
$f_\theta(\iota;\theta) \approx \frac{a_{t+1}}{m_t}$. Furthermore, to
facilitate Monte Carlo simulation, following the literature, we introduce an auxiliary variable $h_\phi(\iota;\phi) \approx \frac{\mathbb{E}_{\epsilon} U'(c_{t+1},h_{t+1})}{U'(c_t,h_t)}\Big\{\, r_t - T'(i_{t+1})\,r_t + 1 - {T^a}'(a_{t+1}) \,\Big\}$ and add the
penalty term $\upsilon_h\!\left(h - b(\cdot)\right)$. Setting $\upsilon_h$
large (or annealing it upward) enforces the equality constraint in the
exact-penalty sense. As a result, we use the following approximation:

\begin{equation}
  \mathbb{E}_{\vec{\iota}}\left[\phi^{\text{FB}}\!\left(
  \frac{a_{t+1}}{m_t},\,
  1 - h_{\phi}
  \right) \right]^2 + 
  \underbrace{\left[ \upsilon_h\!\left(h_{\phi} - \frac{\mathbb{E}_{\vec{\epsilon}} \beta U'(c_{t+1},h_{t+1})}{U'(c_t,h_t)}\Big\{\, r_t - T'(i_{t+1})\,r_t + 1 - {T^a}'(a_{t+1}) \,\Big\}\right) \right]^2}_{\text{penalty factor}}
\end{equation}

By applying the AiO operator, we can rewrite the previous equation as follows:

\begin{equation}
\begin{aligned}
\mathbb{E}_{\vec{\omega}} \Big\{ &
    \left[\,
      \phi^{\text{FB}}\!\left(
        \tfrac{a_{t+1}}{m_t},\,
        1 - h_{\phi}
      \right)
    \right]^2
    \\
    &\quad +\;
    \upsilon_h \Bigg(
      \,h - 
      \frac{\beta U'(c_{t+1},h_{t+1})|_{\vec{\epsilon}=\vec{\epsilon_1}}}
           {U'(c_t,h_t)}
      \Big\{ r_t - T'(i_{t+1}) r_t + 1 - {T^a}'(a_{t+1}) \Big\}
    \Bigg)
    \\
    &\qquad \times
    \Bigg(
      h - 
      \frac{\beta U'(c_{t+1},h_{t+1})|_{\vec{\epsilon}=\vec{\epsilon_2}}}
           {U'(c_t,h_t)}
      \Big\{ r_t - T'(i_{t+1}) r_t + 1 - {T^a}'(a_{t+1}) \Big\}
    \Bigg)
\;\Big\}.
\end{aligned}
\end{equation}