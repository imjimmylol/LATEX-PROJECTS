\section{RESEARCH METHODOLOGY}

We develop a hybrid framework that combines deep-learning solutions to dynamic economic optimization with a targeted reinforcement learning (RL) policy agent. RL is reserved solely for the government, while households (and firms, if included) are modeled as optimizing agents whose decision rules are computed via a neural network (NN) solver that minimizes Euler-equation residuals. This preserves equilibrium discipline and heterogeneity without incurring the instability typical of full multi-agent RL.

\paragraph{Stage 1: Economic environment construction.}
We specify a backbone macroeconomic model with heterogeneous households, preference and technology parameters, market structure, and aggregate shocks. Each household type’s policy function is represented by a neural network $f_{\phi}$ that maps states (idiosyncratic and aggregate) into decisions (e.g., consumption, savings, labor). Rather than learning from demonstrations, $f_{\phi}$ is trained by minimizing Euler residuals implied by agents’ first-order conditions, following \cite{maliar2021deep}. This is akin to an imitation-learning analogue in which the ``expert'' is the model’s optimality conditions rather than data \cite{ho2016generative}. Monte Carlo simulation over the joint state distribution enables scalable training across high-dimensional state spaces and rich heterogeneity.

\paragraph{Stage 2: Equilibrium-consistent environment dynamics.}
Given $f_{\phi}$, we simulate the cross-section of households and aggregate to market-level objects (e.g., factor supplies, demands). Prices or wedges are updated by an aggregator (e.g., market-clearing or reduced-form pricing map), delivering a dynamic, equilibrium-consistent environment that responds to policy changes. We iterate between (i) updating $f_{\phi}$ by minimizing Euler residuals under current prices/policy and (ii) recomputing aggregates until residuals and aggregates stabilize within tolerance.

\paragraph{Stage 3: RL-based policy optimization.}
With a responsive, equilibrium-grounded environment in place, we introduce the government RL agent. 
The complete procedure is summarized in Algorithm~\ref{alg:sams} (see Appendix).

\medskip
\noindent
Overall, the system alternates between (i) solving households’ optimization via Euler-residual minimization and (ii) improving the government’s policy via RL, yielding a Semi-RL Autonomous Macro System that is both scalable and theoretically disciplined.